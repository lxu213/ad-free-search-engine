{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes, Links, Code Snippets During Common Crawl Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "Example Repo: https://github.com/commoncrawl/cc-pyspark\n",
    "\n",
    "Common Crawl Format Example: https://gist.github.com/Smerity/e750f0ef0ab9aa366558#file-bbc-warc\n",
    "\n",
    "Configure EMR to run a pyspark job using Python: https://aws.amazon.com/premiumsupport/knowledge-center/emr-pyspark-python-3x/\n",
    "\n",
    "Apache PySpark Documentation: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "\n",
    "PySpark Cheat Sheet: Spark in Python: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python\n",
    "\n",
    "PySpark Tutorial-Learn to use Apache Spark with Python: https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial\n",
    "\n",
    "Apache Spark: Python Programming Guide: https://spark.apache.org/docs/0.9.0/python-programming-guide.html\n",
    "\n",
    "Open Source Search Engines in Python: http://pythonsource.com/open-source/search-engines\n",
    "\n",
    "Implementing a Search Engine with Ranking in Python: http://aakashjapi.com/fuckin-search-engines-how-do-they-work/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bash Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Point the environment variable SPARK_HOME to your Spark installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ export SPARK_HOME=\"/Users/lxu213/spark/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Submit example job to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ $SPARK_HOME/bin/spark-submit ./server_count.py \\ --num_output_partitions 1 --log_level WARN \\ ./input/test_warc.txt servernames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReadWARC: Assuming that you have the aws command line tools installed, you can list the contents of a crawl using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ aws s3 ls s3://commoncrawl/crawl-data/CC-MAIN-2014-10/ --recursive | head -6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy one segment to local using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ aws s3 cp s3://commoncrawl/crawl-data/CC-MAIN-2014-10/segments/1394023864559/warc/CC-MAIN-20140305125104-00002-ip-10-183-142-35.ec2.internal.warc.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "SparkContext = Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. \n",
    "\n",
    "SQLContext = The entry point for working with structured data (rows and columns) in Spark. Allows the creation of DataFrame objects as well as the execution of SQL queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resilient Distributed Datasets are Apache Spark’s data abstraction, and the features they are built and implemented with are responsible for their significant speed. More about RDDs below:\n",
    "\n",
    "1. RDDs are read-only, partitioned data stores, which are distributed across many machines (typically on a cluster)\n",
    "2. RDDs can be invoked within Spark through Pyspark, Spark SQL or Spark Scala. Data which is ingested, or exists on the disk on the Linux file system or on the Hadoop Distributed File System (HDFS) can be taken and converted to a distributed dataset.\n",
    "3. The key reasons RDDs are an abstraction that works better for distributed data processing, is because they don’t feature some of the issues that MapReduce, the older paradigm for data processing (which Spark is replacing increasingly). Chiefly, these are:\n",
    "    - Replication: Replication of data on different parts of a cluster, is a feature of HDFS that enables data to be stored in a fault-tolerant manner. Spark’s RDDs address fault tolerance by using a lineage graph. The different name (resilient, as opposed to replicated) indicates this difference of implementation in the core functionality of Spark\n",
    "    - Serialization: Serialization in MapReduce bogs it down, speed wise, in operations like shuffling and sorting.\n",
    "    - Disk IO : One of the most computationally expensive operations is writing files to disk and reading them again, and this kind of Disk input-output impacts the performance of big compute jobs. Although Apache Spark can cache and persist RDDs to save time during in-memory computation, it is primarily an in-memory processing engine that depends on cheap access to RAM (which differs from the “commodity hardware” argument that’s made for Hadoop). Disk IO is expensive and time consuming in “big compute” jobs (as opposed to “big data”, which refers to large data set storage and handling). At every stage of a map or reduce step in MapReduce, there is Disk IO, which is avoided because Spark’s resource manager and optimiser allow for fine-grained control over scheduling and resilient processing.\n",
    "    - Optimisation and Lazy Evaluation: These are mentioned together since lazy evaluation (a la Scala) allows a sequence of transformations to be performed on RDDs without actually spending compute time on them. Spark natively represents these transformations as a Directed Acyclic Graph (DAG) and Spark’s Catalyst Optimizer allows such computational graphs to be optimised and staged appropriately, based on the memory settings. Spark’s native resource manager is capable of handling various tasks by itself in conjunction with a file system, but Spark also integrates with existing resource managers in Hadoop based file systems (such as YARN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reading WARC Records\n",
    "\n",
    "A key feature of the library is to be able to iterate over a stream of WARC records using the ArchiveIterator\n",
    "\n",
    "It includes the following features: - Reading a WARC/ARC stream - On the fly ARC to WARC record conversion - Decompressing and de-chunking HTTP payload content stored in WARC/ARC files.\n",
    "\n",
    "For example, the following prints the the url for each WARC response record:\n",
    "\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "with open('path/to/file', 'rb') as stream:\n",
    "    for record in ArchiveIterator(stream):\n",
    "        if record.rec_type == 'response':\n",
    "            print(record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "The stream object could be a file on disk or a remote network stream. The ArchiveIterator reads the WARC content in a single pass. The record is represented by an ArcWarcRecord object which contains the format (ARC or WARC), record type, the record headers, http headers (if any), and raw stream for reading the payload.\n",
    "\n",
    "class ArcWarcRecord(object):\n",
    "    def __init__(self, *args):\n",
    "        (self.format, self.rec_type, self.rec_headers, self.raw_stream,\n",
    "         self.http_headers, self.content_type, self.length) = args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Questions\n",
    "1. \"The key reasons RDDs are an abstraction that works better for distributed data processing, is because they don’t feature some of the issues that MapReduce\" ... MR is a strategy that can also be used in Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Keywords Python Function\n",
    "\n",
    "Inherits from `CCSparkJob` and can run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run extract_keyword.py in shell\n",
    "$ cd data/ad-free-search-engine\n",
    "$ python extract_keyword.py input/test_wat.txt output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>al.hoteleriturizemalbania</td>\n",
       "      <td>al.gov.asha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>al.hoteleriturizemalbania</td>\n",
       "      <td>com.albaniantourism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>al.hoteleriturizemalbania</td>\n",
       "      <td>info.pogradec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>al.hoteleriturizemalbania</td>\n",
       "      <td>org.kelmend-shkrel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>am.do.rofiler</td>\n",
       "      <td>com.mozilla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           s                    t\n",
       "5  al.hoteleriturizemalbania          al.gov.asha\n",
       "6  al.hoteleriturizemalbania  com.albaniantourism\n",
       "7  al.hoteleriturizemalbania        info.pogradec\n",
       "8  al.hoteleriturizemalbania   org.kelmend-shkrel\n",
       "9              am.do.rofiler          com.mozilla"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract links wat \n",
    "parquet_path = '/Users/lxu213/data/cc-pyspark-master/spark-warehouse/wat_extract_link_output/part-00000-cbd63fe7-1bb9-4dab-a3a5-727cd8154dd5-c000.snappy.parquet'\n",
    "table_wat = pq.read_table(parquet_path, nthreads=4).to_pandas()\n",
    "table_wat[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://0lik.ru/cliparts/clipartfoto/128020-sto...</td>\n",
       "      <td>stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://0lik.ru/cliparts/clipartfoto/128020-sto...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://0lik.ru/cliparts/clipartfoto/128020-sto...</td>\n",
       "      <td>panoramas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://0lik.ru/cliparts/clipartfoto/128020-sto...</td>\n",
       "      <td>european</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://0lik.ru/cliparts/clipartfoto/128020-sto...</td>\n",
       "      <td>cities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   keywords\n",
       "0  http://0lik.ru/cliparts/clipartfoto/128020-sto...      stock\n",
       "1  http://0lik.ru/cliparts/clipartfoto/128020-sto...      photo\n",
       "2  http://0lik.ru/cliparts/clipartfoto/128020-sto...  panoramas\n",
       "3  http://0lik.ru/cliparts/clipartfoto/128020-sto...   european\n",
       "4  http://0lik.ru/cliparts/clipartfoto/128020-sto...     cities"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract keywords\n",
    "kw_path = '/Users/lxu213/data/ad-free-search-engine/spark-warehouse/output/part-00000-06625965-361d-4688-8f64-0784d2384653-c000.snappy.parquet'\n",
    "data = pq.read_table(kw_path, nthreads=4).to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>153823</td>\n",
       "      <td>153823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>29704</td>\n",
       "      <td>42167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>http://www.juegostin.com/jugar/jugar-de-cestir...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>20</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url keywords\n",
       "count                                              153823   153823\n",
       "unique                                              29704    42167\n",
       "top     http://www.juegostin.com/jugar/jugar-de-cestir...       de\n",
       "freq                                                   20     1047"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Hello World!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
